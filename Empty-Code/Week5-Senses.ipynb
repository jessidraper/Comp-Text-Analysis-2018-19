{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('31', 'CD'), (',', ','), ('2017', 'CD'), ('In', 'IN'), ('Sydney', 'NNP'), (',', ','), ('rainbow', 'NN'), ('fireworks', 'NNS'), ('sparkled', 'VBD'), ('off', 'RP'), ('the', 'DT'), ('Harbour', 'NNP'), ('Bridge', 'NNP'), ('in', 'IN'), ('celebration', 'NN'), ('of', 'IN'), ('Australia', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('recent', 'JJ'), ('legalization', 'NN'), ('of', 'IN'), ('gay', 'JJ'), ('marriage', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# open the new dataset\n",
    "\n",
    "import codecs, nltk\n",
    "\n",
    "article = codecs.open(\"../datasets/CleanedArticles/15.txt\",\"r\",\"utf-8\")\n",
    "article = article.read()\n",
    "\n",
    "# split into sentences\n",
    "sentences = nltk.sent_tokenize(article) \n",
    "\n",
    "# take one single sentence \n",
    "\n",
    "sentence = sentences[1]\n",
    "\n",
    "#tokenize it\n",
    "\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "# you use the pos-tagger (it gives you back a list of tuples (word,pos))\n",
    "pos_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "print (pos_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', ',', '2017', 'In', 'Sydney', ',', 'rainbow', 'firework', 'sparkle', 'off', 'the', 'Harbour', 'Bridge', 'in', 'celebration', 'of', 'Australia', '’', 's', 'recent', 'legalization', 'of', 'gay', 'marriage', '.']\n"
     ]
    }
   ],
   "source": [
    "# combining lemmatization and pos tagging\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_words = []\n",
    "\n",
    "for word,pos in pos_sentence:\n",
    "    \n",
    "    # if it's a verb - then we tell that to the lemmatizer\n",
    "    if pos[0] == \"V\":\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word,\"v\")\n",
    "    else:\n",
    "    # otherwise, work as usual\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "    # we append the results\n",
    "    lemma_words.append(lemma)\n",
    "    \n",
    "print (lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's now define a function that does all we need\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token,\"v\") if pos[0] == \"V\" else wordnet_lemmatizer.lemmatize(token) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage.\n",
      "['In', 'Sydney', 'rainbow', 'firework', 'sparkle', 'Harbour', 'Bridge', 'celebration', 'Australia', 'recent', 'legalization', 'gay', 'marriage']\n"
     ]
    }
   ],
   "source": [
    "print (sentence)\n",
    "clean_sentence = nlp_pipeline(sentence)\n",
    "print (clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Advertisement', 'By', 'JACEY', 'FORTINDEC', 'In', 'Sydney', 'rainbow', 'firework', 'sparkle', 'Harbour', 'Bridge', 'celebration', 'Australia', 'recent', 'legalization', 'gay', 'marriage', 'Sydney', 'among', 'first', 'major', 'city', 'celebrate', 'firework', 'stroke', 'midnight', 'In', 'Japan', 'people', 'parade', 'fox', 'mask', 'attend', 'first', 'prayer', 'year', 'Shinto', 'shrine', 'Tokyo', 'In', 'Philippines', 'reveler', 'gather', 'phone', 'hand', 'Eastwood', 'Mall', 'Manila', 'watch', 'balloon', 'confetti', 'rain', 'midnight', 'Big', 'pot', 'tea', 'prepare', 'New', 'Year', 'Eve', 'celebration', 'Beijing', 'The', 'country', 'also', 'celebrate', 'Lunar', 'New', 'Year', 'February', 'It', 'rain', 'Singapore', 'New', 'Year', 'Eve', 'celebrant', 'shelter', 'umbrella', 'raincoat', 'firework', 'sparkle', 'overhead', 'Tourists', 'party', 'hat', 'watch', 'firework', 'front', 'famous', 'Petronas', 'Twin', 'Towers', 'Kuala', 'Lumpur', 'Malaysia', 'Hundreds', 'couple', 'get', 'marry', 'mass', 'wedding', 'Jakarta', 'New', 'Year', 'Eve', 'We', 'interested', 'feedback', 'page', 'Tell', 'u', 'think', 'Go', 'Home', 'Page']\n"
     ]
    }
   ],
   "source": [
    "# let's take an entire article and use our pipeline!\n",
    "\n",
    "clean_article = nlp_pipeline(article)\n",
    "print (clean_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couple 9\n",
      "The 0\n",
      "Lumpur 0\n",
      "Harbour 6\n",
      "gay 7\n",
      "feedback 2\n",
      "also 1\n",
      "celebration 3\n",
      "confetti 1\n",
      "Kuala 0\n",
      "firework 1\n",
      "Tourists 1\n",
      "think 14\n",
      "New 12\n",
      "Malaysia 1\n",
      "Go 35\n",
      "We 0\n",
      "people 6\n",
      "attend 5\n",
      "reveler 1\n",
      "Twin 9\n",
      "It 1\n",
      "Beijing 1\n",
      "sparkle 7\n",
      "Eve 4\n",
      "mass 11\n",
      "hat 4\n",
      "midnight 1\n",
      "famous 1\n",
      "city 3\n",
      "Page 9\n",
      "phone 4\n",
      "among 0\n",
      "hand 16\n",
      "tea 5\n",
      "Manila 2\n",
      "Tell 9\n",
      "legalization 1\n",
      "Advertisement 1\n",
      "shrine 2\n",
      "get 37\n",
      "Tokyo 1\n",
      "Hundreds 1\n",
      "rain 4\n",
      "celebrate 3\n",
      "gather 11\n",
      "fox 10\n",
      "Towers 4\n",
      "Japan 5\n",
      "stroke 16\n",
      "shelter 7\n",
      "Home 17\n",
      "rainbow 2\n",
      "pot 10\n",
      "Bridge 12\n",
      "balloon 4\n",
      "major 13\n",
      "first 16\n",
      "Sydney 1\n",
      "In 7\n",
      "recent 3\n",
      "Petronas 0\n",
      "Year 4\n",
      "Singapore 3\n",
      "celebrant 2\n",
      "page 9\n",
      "wedding 5\n",
      "prayer 5\n",
      "marry 2\n",
      "Australia 2\n",
      "Philippines 3\n",
      "u 4\n",
      "raincoat 1\n",
      "prepare 8\n",
      "year 4\n",
      "By 2\n",
      "watch 13\n",
      "Mall 2\n",
      "umbrella 4\n",
      "interested 5\n",
      "overhead 9\n",
      "Jakarta 1\n",
      "Big 17\n",
      "Lunar 1\n",
      "country 5\n",
      "February 1\n",
      "Eastwood 0\n",
      "JACEY 0\n",
      "FORTINDEC 0\n",
      "party 6\n",
      "Shinto 3\n",
      "marriage 4\n",
      "parade 5\n",
      "front 13\n",
      "mask 9\n"
     ]
    }
   ],
   "source": [
    "# word sense disambiguation\n",
    "\n",
    "# check documentation: http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# let's isolate each word - you do this using a set (another type of object in python)\n",
    "\n",
    "unique_words = set(clean_article)\n",
    "# let's check how many senses each word has\n",
    "for word in unique_words:\n",
    "    print (word, len(wn.synsets(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "definition\n",
      "any small compartment\n",
      "\n",
      "example\n",
      "['the cells of a honeycomb']\n",
      "\n",
      "hypernymy\n",
      "[Synset('compartment.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[]\n",
      "\n",
      "synonyms\n",
      "['cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "(biology) the basic structural and functional unit of all organisms; they may exist as independent units of life (as in monads) or may form colonies or tissues as in higher plants and animals\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('living_thing.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('akaryocyte.n.01'), Synset('archespore.n.01'), Synset('arthrospore.n.01'), Synset('arthrospore.n.02'), Synset('beta_cell.n.01'), Synset('blastema.n.01'), Synset('blastomere.n.01'), Synset('daughter_cell.n.01'), Synset('embryonic_cell.n.01'), Synset('fiber.n.03'), Synset('flagellated_cell.n.01'), Synset('gametocyte.n.01'), Synset('kupffer's_cell.n.01'), Synset('leydig_cell.n.01'), Synset('mother_cell.n.01'), Synset('parthenote.n.01'), Synset('plant_cell.n.01'), Synset('polar_body.n.01'), Synset('recombinant.n.01'), Synset('reproductive_cell.n.01'), Synset('sertoli_cell.n.01'), Synset('somatic_cell.n.01'), Synset('zygote.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a device that delivers an electric current as the result of a chemical reaction\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('electrical_device.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('electrolytic_cell.n.01'), Synset('fuel_cell.n.01'), Synset('solar_cell.n.01'), Synset('storage_cell.n.01'), Synset('voltaic_cell.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell', 'electric_cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a small unit serving as part of or as the nucleus of a larger political movement\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('political_unit.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('sleeper_cell.n.01'), Synset('terrorist_cell.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell', 'cadre']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a hand-held mobile radiotelephone for use in an area divided into small sections, each with its own short-range transmitter/receiver\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('radiotelephone.n.02')]\n",
      "\n",
      "hyponyms\n",
      "[]\n",
      "\n",
      "synonyms\n",
      "['cellular_telephone', 'cellular_phone', 'cellphone', 'cell', 'mobile_phone']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "small room in which a monk or nun lives\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('room.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[]\n",
      "\n",
      "synonyms\n",
      "['cell', 'cubicle']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a room where a prisoner is kept\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('room.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('bullpen.n.02'), Synset('cooler.n.03'), Synset('dungeon.n.02'), Synset('guardroom.n.01'), Synset('hold.n.07'), Synset('sweatbox.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell', 'jail_cell', 'prison_cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    print (\"\\ndefinition\")\n",
    "    print(sense.definition())\n",
    "    \n",
    "    # get a textual example\n",
    "    print (\"\\nexample\")\n",
    "    print(sense.examples())\n",
    "    \n",
    "    # get hypernymy\n",
    "    print (\"\\nhypernymy\")\n",
    "    print(sense.hypernyms())\n",
    "\n",
    "    # get hyponyms\n",
    "    print (\"\\nhyponyms\")\n",
    "    print(sense.hyponyms())\n",
    "        \n",
    "    # this is a way of getting synonyms - there are others\n",
    "    print (\"\\nsynonyms\")\n",
    "    print (sense.lemma_names())\n",
    "    \n",
    "    # this is for getting antonyms - works especially with adjectives \n",
    "    print (\"\\nantonyms\")\n",
    "    print (sense.lemmas()[0].antonyms())\n",
    "    \n",
    "    print (\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sent 1: ['The', 'terrorist', 'cell', 'neutralize', 'near', 'southern', 'Russian', 'city', 'Makhachkala', 'capital', 'Republic', 'Dagestan']\n",
      "clean sent 2: ['The', 'molecule', 'use', 'light', 'energy', 'move', 'proton', 'across', 'somatic', 'cell', 'membrane', 'prove', 'unsuitable', 'crystallography']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# let's consider two sentences where \"cell\" is mentioned\n",
    "\n",
    "sent1 = \"The terrorist cell was neutralized near the southern Russian city of Makhachkala, the capital of the Republic of Dagestan.\"\n",
    "\n",
    "sent2 = \"The molecule, which uses light energy to move protons across a somatic cell membrane, proved unsuitable for crystallography.\"\n",
    "\n",
    "# you clean the sentences using our pipeline\n",
    "clean_sent1 = nlp_pipeline(sent1)\n",
    "\n",
    "clean_sent2 = nlp_pipeline(sent2)\n",
    "\n",
    "print (\"clean sent 1:\", clean_sent1)\n",
    "print (\"clean sent 2:\", clean_sent2)\n",
    "print (\" \")\n",
    "\n",
    "# for each possible sense of \"cell\" you can, for instance, check the overlap between the definition and the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "–\n",
    "for sense in senses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# homework: find the best sense - implement your version of the Lesk algorithm: https://en.wikipedia.org/wiki/Lesk_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "homework 2: get the json file with the tweets from Donald Trump and improve his vocabulary by changing his poor choice of adjectives with more sophisticated synonyms (e.g. \"bad ratings on the Emmys last night\" -> \"substandard ratings on the Emmys last night\") \n",
    " \n",
    "or\n",
    "\n",
    "make his tweets nicer by changing adjectives with related antonyms (e.g. \"bad ratings on the Emmys last night\" -> \"excellent ratings on the Emmys last night\") \n",
    "\n",
    "to do you need to combine:\n",
    "\n",
    "- text processing (POS tagging + WordNet)\n",
    "- and to find a solution for knowing if a word is \"more sophisticated\" than another one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
