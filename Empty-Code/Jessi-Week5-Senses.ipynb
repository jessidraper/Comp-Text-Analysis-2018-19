{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('31', 'CD'), (',', ','), ('2017', 'CD'), ('In', 'IN'), ('Sydney', 'NNP'), (',', ','), ('rainbow', 'NN'), ('fireworks', 'NNS'), ('sparkled', 'VBD'), ('off', 'RP'), ('the', 'DT'), ('Harbour', 'NNP'), ('Bridge', 'NNP'), ('in', 'IN'), ('celebration', 'NN'), ('of', 'IN'), ('Australia', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('recent', 'JJ'), ('legalization', 'NN'), ('of', 'IN'), ('gay', 'JJ'), ('marriage', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# open the new dataset\n",
    "\n",
    "import codecs, nltk\n",
    "\n",
    "article = codecs.open(\"../datasets/CleanedArticles/15.txt\",\"r\",\"utf-8\")\n",
    "article = article.read()\n",
    "\n",
    "# split into sentences\n",
    "sentences = nltk.sent_tokenize(article) \n",
    "\n",
    "# take one single sentence \n",
    "\n",
    "sentence = sentences[1]\n",
    "\n",
    "#tokenize it\n",
    "\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "# you use the pos-tagger (it gives you back a list of tuples (word,pos))\n",
    "pos_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "print (pos_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', ',', '2017', 'In', 'Sydney', ',', 'rainbow', 'firework', 'sparkle', 'off', 'the', 'Harbour', 'Bridge', 'in', 'celebration', 'of', 'Australia', '’', 's', 'recent', 'legalization', 'of', 'gay', 'marriage', '.']\n"
     ]
    }
   ],
   "source": [
    "# combining lemmatization and pos tagging\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_words = []\n",
    "\n",
    "for word,pos in pos_sentence:\n",
    "    \n",
    "    # if it's a verb - then we tell that to the lemmatizer\n",
    "    if pos[0] == \"V\":\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word,\"v\")\n",
    "    else:\n",
    "    # otherwise, work as usual\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "    # we append the results\n",
    "    lemma_words.append(lemma)\n",
    "    \n",
    "print (lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's now define a function that does all we need\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\") if pos[0] == \"V\" else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sydney', 'rainbow', 'firework', 'sparkle', 'harbour', 'bridge', 'celebration', 'australia', 'recent', 'legalization', 'gay', 'marriage']\n"
     ]
    }
   ],
   "source": [
    "clean_sentence = nlp_pipeline(sentence)\n",
    "print (clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advertisement', 'jacey', 'fortindec', 'sydney', 'rainbow', 'firework', 'sparkle', 'harbour', 'bridge', 'celebration', 'australia', 'recent', 'legalization', 'gay', 'marriage', 'sydney', 'among', 'first', 'major', 'city', 'celebrate', 'firework', 'stroke', 'midnight', 'japan', 'people', 'parade', 'fox', 'mask', 'attend', 'first', 'prayer', 'year', 'shinto', 'shrine', 'tokyo', 'philippine', 'reveler', 'gather', 'phone', 'hand', 'eastwood', 'mall', 'manila', 'watch', 'balloon', 'confetti', 'rain', 'midnight', 'big', 'pot', 'tea', 'prepare', 'new', 'year', 'eve', 'celebration', 'beijing', 'country', 'also', 'celebrate', 'lunar', 'new', 'year', 'february', 'rain', 'singapore', 'new', 'year', 'eve', 'celebrant', 'shelter', 'umbrella', 'raincoat', 'firework', 'sparkle', 'overhead', 'tourist', 'party', 'hat', 'watch', 'firework', 'front', 'famous', 'petronas', 'twin', 'tower', 'kuala', 'lumpur', 'malaysia', 'hundred', 'couple', 'get', 'marry', 'mass', 'wedding', 'jakarta', 'new', 'year', 'eve', 'interested', 'feedback', 'page', 'tell', 'u', 'think', 'go', 'home', 'page']\n"
     ]
    }
   ],
   "source": [
    "# let's take an entire article and use our pipeline!\n",
    "\n",
    "clean_article = nlp_pipeline(article)\n",
    "print (clean_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "{'australia', 'sydney', 'sparkle', 'marriage', 'mask', 'front', 'eve', 'marry', 'go', 'prayer', 'singapore', 'harbour', 'watch', 'reveler', 'celebrate', 'big', 'gather', 'gay', 'jacey', 'feedback', 'page', 'balloon', 'phone', 'philippine', 'tourist', 'recent', 'kuala', 'party', 'among', 'rain', 'fortindec', 'get', 'japan', 'umbrella', 'hat', 'celebrant', 'year', 'eastwood', 'u', 'manila', 'advertisement', 'february', 'wedding', 'parade', 'prepare', 'shrine', 'shinto', 'lunar', 'interested', 'jakarta', 'new', 'celebration', 'home', 'tokyo', 'overhead', 'midnight', 'bridge', 'fox', 'confetti', 'mass', 'hundred', 'twin', 'think', 'malaysia', 'also', 'mall', 'city', 'stroke', 'attend', 'petronas', 'couple', 'legalization', 'beijing', 'tea', 'lumpur', 'rainbow', 'country', 'pot', 'shelter', 'tower', 'first', 'raincoat', 'tell', 'hand', 'firework', 'people', 'famous', 'major'}\n"
     ]
    }
   ],
   "source": [
    "print(type(test))\n",
    "test = set(clean_article)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia 2\n",
      "sydney 1\n",
      "sparkle 7\n",
      "marriage 4\n",
      "mask 9\n",
      "front 13\n",
      "eve 4\n",
      "marry 2\n",
      "go 35\n",
      "prayer 5\n",
      "singapore 3\n",
      "harbour 6\n",
      "watch 13\n",
      "reveler 1\n",
      "celebrate 3\n",
      "big 17\n",
      "gather 11\n",
      "gay 7\n",
      "jacey 0\n",
      "feedback 2\n",
      "page 9\n",
      "balloon 4\n",
      "phone 4\n",
      "philippine 2\n",
      "tourist 1\n",
      "recent 3\n",
      "kuala 0\n",
      "party 6\n",
      "among 0\n",
      "rain 4\n",
      "fortindec 0\n",
      "get 37\n",
      "japan 5\n",
      "umbrella 4\n",
      "hat 4\n",
      "celebrant 2\n",
      "year 4\n",
      "eastwood 0\n",
      "u 4\n",
      "manila 2\n",
      "advertisement 1\n",
      "february 1\n",
      "wedding 5\n",
      "parade 5\n",
      "prepare 8\n",
      "shrine 2\n",
      "shinto 3\n",
      "lunar 1\n",
      "interested 5\n",
      "jakarta 1\n",
      "new 12\n",
      "celebration 3\n",
      "home 17\n",
      "tokyo 1\n",
      "overhead 9\n",
      "midnight 1\n",
      "bridge 12\n",
      "fox 10\n",
      "confetti 1\n",
      "mass 11\n",
      "hundred 2\n",
      "twin 9\n",
      "think 14\n",
      "malaysia 1\n",
      "also 1\n",
      "mall 2\n",
      "city 3\n",
      "stroke 16\n",
      "attend 5\n",
      "petronas 0\n",
      "couple 9\n",
      "legalization 1\n",
      "beijing 1\n",
      "tea 5\n",
      "lumpur 0\n",
      "rainbow 2\n",
      "country 5\n",
      "pot 10\n",
      "shelter 7\n",
      "tower 4\n",
      "first 16\n",
      "raincoat 1\n",
      "tell 9\n",
      "hand 16\n",
      "firework 1\n",
      "people 6\n",
      "famous 1\n",
      "major 13\n"
     ]
    }
   ],
   "source": [
    "# word sense disambiguation\n",
    "\n",
    "# check documentation: http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# let's isolate each word - you do this using a set (another type of object in python)\n",
    "\n",
    "unique_words = set(clean_article)\n",
    "\n",
    "# let's check how many senses each word has\n",
    "for word in unique_words:\n",
    "    print (word, len(wn.synsets(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "definition\n",
      "the temporary provision of money (usually at interest)\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('debt.n.02')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('bank_loan.n.01'), Synset('call_loan.n.01'), Synset('direct_loan.n.01'), Synset('home_loan.n.01'), Synset('installment_credit.n.01'), Synset('participation_loan.n.01'), Synset('personal_loan.n.01'), Synset('point.n.18'), Synset('real_estate_loan.n.01'), Synset('time_loan.n.01')]\n",
      "\n",
      "synonyms\n",
      "['loan']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a word borrowed from another language; e.g. `blitz' is a German word borrowed into modern English\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('word.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('gallicism.n.01'), Synset('latinism.n.01')]\n",
      "\n",
      "synonyms\n",
      "['loanword', 'loan']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "give temporarily; let have for a limited time\n",
      "\n",
      "example\n",
      "['I will lend you my car', 'loan me some money']\n",
      "\n",
      "hypernymy\n",
      "[Synset('give.v.03')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('advance.v.10'), Synset('hire_out.v.01'), Synset('trust.v.06')]\n",
      "\n",
      "synonyms\n",
      "['lend', 'loan']\n",
      "\n",
      "antonyms\n",
      "[Lemma('borrow.v.01.borrow')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"loan\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    print (\"\\ndefinition\")\n",
    "    print(sense.definition())\n",
    "    \n",
    "    # get a textual example\n",
    "    print (\"\\nexample\")\n",
    "    print(sense.examples())\n",
    "    \n",
    "    # get hypernymy\n",
    "    print (\"\\nhypernymy\")\n",
    "    print(sense.hypernyms())\n",
    "\n",
    "    # get hyponyms\n",
    "    print (\"\\nhyponyms\")\n",
    "    print(sense.hyponyms())\n",
    "        \n",
    "    # this is a way of getting synonyms - there are others\n",
    "    print (\"\\nsynonyms\")\n",
    "    print (sense.lemma_names())\n",
    "    \n",
    "    # this is for getting antonyms - works especially with adjectives \n",
    "    print (\"\\nantonyms\")\n",
    "    print (sense.lemmas()[0].antonyms())\n",
    "    \n",
    "    print (\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sent 1: ['terrorist', 'cell', 'neutralize', 'near', 'southern', 'russian', 'city', 'makhachkala', 'capital', 'republic', 'dagestan']\n",
      "clean sent 2: ['molecule', 'use', 'light', 'energy', 'move', 'proton', 'across', 'somatic', 'cell', 'membrane', 'prove', 'unsuitable', 'crystallography']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# let's consider two sentences where \"cell\" is mentioned\n",
    "\n",
    "sent1 = \"The terrorist cell was neutralized near the southern Russian city of Makhachkala, the capital of the Republic of Dagestan.\"\n",
    "\n",
    "sent2 = \"The molecule, which uses light energy to move protons across a somatic cell membrane, proved unsuitable for crystallography.\"\n",
    "\n",
    "# you clean the sentences using our pipeline\n",
    "clean_sent1 = nlp_pipeline(sent1)\n",
    "\n",
    "clean_sent2 = nlp_pipeline(sent2)\n",
    "\n",
    "print (\"clean sent 1:\", clean_sent1)\n",
    "print (\"clean sent 2:\", clean_sent2)\n",
    "print (\" \")\n",
    "\n",
    "# for each possible sense of \"cell\" you can, for instance, check the overlap between the definition and the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('compartment.n.01')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'definition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-383f5aef0b0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#print(hyponyms)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'definition'"
     ]
    }
   ],
   "source": [
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    \n",
    "    # get definition of sense\n",
    "    definition =  sense.definition()\n",
    "    \n",
    "    # get hypernymy\n",
    "    hypernyms = sense.hypernyms()\n",
    "\n",
    "    # get hyponyms\n",
    "    hyponyms = sense.hyponyms()\n",
    "    \n",
    "    print(hypernyms)\n",
    "    #print(hyponyms)\n",
    "    print(hypernyms.definition())\n",
    "    break\n",
    "    \n",
    "    # you clean the definition with our pipeline\n",
    "    #clean_definition = nlp_pipeline(definition)\n",
    "    \n",
    "    # you check the intersection of the two sentences\n",
    "   # inters_1 = set(clean_sent1).intersection(clean_definition)\n",
    "    #inters_2 = set(clean_sent2).intersection(clean_definition)\n",
    "    \n",
    "    #print (definition)\n",
    "    #print (\"clean definition:\", clean_definition)\n",
    "    #print (\"intersection with sent 1:\", inters_1)\n",
    "    #print (\"intersection with sent 2:\", inters_2)\n",
    "    #print (len(inters_1),len(inters_2))\n",
    "    #print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    \n",
    "    # get definition of sense\n",
    "    definition =  sense.definition()\n",
    "    \n",
    "    # you clean the definition with our pipeline\n",
    "    clean_definition = nlp_pipeline(definition)\n",
    "    \n",
    "    # you check the intersection of the two sentences\n",
    "    inters_1 = set(clean_sent1).intersection(clean_definition)\n",
    "    inters_2 = set(clean_sent2).intersection(clean_definition)\n",
    "    \n",
    "    print (definition)\n",
    "    print (\"clean definition:\", clean_definition)\n",
    "    print (\"intersection with sent 1:\", inters_1)\n",
    "    print (\"intersection with sent 2:\", inters_2)\n",
    "    print (len(inters_1),len(inters_2))\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# homework: find the best sense - implement your version of the Lesk algorithm: https://en.wikipedia.org/wiki/Lesk_algorithm\n",
    "\n",
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    \n",
    "    # get definition of sense\n",
    "    definition =  sense.definition()\n",
    "    \n",
    "    # you clean the definition with our pipeline\n",
    "    clean_definition = nlp_pipeline(definition)\n",
    "    \n",
    "    # you check the intersection of the two sentences\n",
    "    inters_1 = set(clean_sent1).intersection(clean_definition)\n",
    "    inters_2 = set(clean_sent2).intersection(clean_definition)\n",
    "    \n",
    "    print (definition)\n",
    "    print (\"clean definition:\", clean_definition)\n",
    "    print (\"intersection with sent 1:\", inters_1)\n",
    "    print (\"intersection with sent 2:\", inters_2)\n",
    "    print (len(inters_1),len(inters_2))\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "homework 2: get the json file with the tweets from Donald Trump and improve his vocabulary by changing his poor choice of adjectives with more sophisticated synonyms (e.g. \"bad ratings on the Emmys last night\" -> \"substandard ratings on the Emmys last night\") \n",
    " \n",
    "or\n",
    "\n",
    "make his tweets nicer by changing adjectives with related antonyms (e.g. \"bad ratings on the Emmys last night\" -> \"excellent ratings on the Emmys last night\") \n",
    "\n",
    "to do you need to combine:\n",
    "\n",
    "- text processing (POS tagging + WordNet)\n",
    "- and to find a solution for knowing if a word is \"more sophisticated\" than another one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35018\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('../datasets/trump.json') as f:\n",
    "    tweets = json.load(f)\n",
    "\n",
    "print (len(tweets))\n",
    "print(type(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'labor', 'day', 'country', 'better', 'ever', 'unemployment', 'setting', 'record', 'low', 'tremendous', 'upside', 'potential', 'go', 'fix', 'worst', 'trade', 'deal', 'ever', 'make', 'country', 'world', 'big', 'progress', 'make']\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    if \"worst\" in tweet[\"text\"]:\n",
    "        clean_tweet = nlp_pipeline(tweet[\"text\"])\n",
    "        print(clean_tweet)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst = wn.synset('worst.a.01')\n",
    "syn = worst.lemmas()[0].antonyms()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
