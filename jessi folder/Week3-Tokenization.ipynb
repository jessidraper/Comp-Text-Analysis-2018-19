{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when should we use a list or a dictionary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW continued .... \n",
    "\n",
    "HW: How many tweets are written by Donald Trump that contain the word \"great\"? \n",
    "\n",
    "TIP: Surpringly, the president of United State shares his twitter account with other people in the White House. Then how do we know whether a tweet if by Donald Trump or by others? Well, scientist later found out that only he uses Samsung phone (whereas others use i-phone) and he usually tweets in morning. \n",
    "\n",
    "Like in the last week, we need a specific package/library called 'json' to get easy access to the \".json\" data. We can import this library with below code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35018\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../datasets/trump.json') as f:\n",
    "    tweets = json.load(f)\n",
    "print (len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375 0.15349248957678907 0.3695427982124441\n"
     ]
    }
   ],
   "source": [
    "written_by_trump = 0   ## <-- our quantity of interest? \n",
    "with_great = 0         ## <-- our quantity of interest? \n",
    "\n",
    "for tweet in tweets: \n",
    "    if \"Android\" in tweet[\"source\"]:\n",
    "        written_by_trump += 1\n",
    "    text = tweet[\"text\"] \n",
    "    if \"great\" in text.lower(): \n",
    "        with_great += 1 \n",
    "        \n",
    "print (with_great, with_great/len(tweets),with_great/written_by_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things-to-do: \n",
    "\n",
    "find all the text files in a designated folder, \n",
    "and retrieve the first 10 words from each text file. \n",
    "\n",
    "\n",
    "## What we need & ways to do: OS, listdir \n",
    "\n",
    "\"os\" is another commonly-used module/package in Python. This is to loop over the files in a folder. \n",
    "\"listdir\" is a function - within the OS module - to list/show all the files that are saved in a designated pathway. \n",
    "\n",
    "Below codes are read as... \n",
    "    1. import a special module/library 'os' (install packages) \n",
    "    2. for all the dataset in this space (designated pathway)\n",
    "    3. if there is dataset with \".txt\" \n",
    "    4. then print that dataset \n",
    "    5. open all the dataset / read the contents within the dataset\n",
    "       by now, each text file is read as if it is a one entity\n",
    "    6. get/read the list 'doc'\n",
    "    7. split the content by blanks, till the 10th words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt\n",
      "['In', 'ways', 'that', 'were', 'once', 'unimaginable,', 'President', 'Trump', 'has', 'discarded']\n",
      " \n",
      "1.txt\n",
      "['In', 'ways', 'that', 'were', 'once', 'unimaginable,', 'President', 'Trump', 'has', 'discarded']\n",
      " \n",
      "10.txt\n",
      "['Advertisement', 'By', 'VIVIAN', 'WANGDEC.', '30,', '2017', '', 'Erica', 'Garner,', 'the']\n",
      " \n",
      "11.txt\n",
      "['Advertisement', 'By', 'MELISSA', 'EDDYDEC.', '31,', '2017', '', 'BERLIN', '—', 'Angela']\n",
      " \n",
      "12.txt\n",
      "['Advertisement', 'By', 'NIKI', 'KITSANTONISDEC.', '30,', '2017', '', 'ATHENS', '—', 'An']\n",
      " \n",
      "13.txt\n",
      "['Advertisement', 'By', 'STEVE', 'WEMBI', 'and', 'JINA', 'MOOREDEC.', '30,', '2017', '']\n",
      " \n",
      "15.txt\n",
      "['Advertisement', 'By', 'JACEY', 'FORTINDEC.', '31,', '2017', '', 'In', 'Sydney,', 'rainbow']\n",
      " \n",
      "16.txt\n",
      "['Advertisement', 'By', 'LOUIS', 'LUCERO', 'II', 'and', 'LANCE', 'BOOTHDEC.', '31,', '2017']\n",
      " \n",
      "18.txt\n",
      "['Advertisement', '', '', '', '', '', '', '', '', 'By']\n",
      " \n",
      "19.txt\n",
      "['Advertisement', '', '', '', '', '', '', '', '', 'By']\n",
      " \n",
      "2.txt\n",
      "['Advertisement', 'By', 'MARTIN', 'FACKLER', 'and', 'RICK', 'GLADSTONEDEC.', '31,', '2017', '']\n",
      " \n",
      "3.txt\n",
      "['Advertisement', 'By', 'CHRISTOPHER', 'MELE', 'and', 'JACK', 'HEALYDEC.', '31,', '2017', '']\n",
      " \n",
      "4.txt\n",
      "['Advertisement', 'By', 'SHARON', 'LaFRANIERE,', 'MARK', 'MAZZETTI', 'and', 'MATT', 'APUZZODEC.', '30,']\n",
      " \n",
      "5.txt\n",
      "['Advertisement', 'By', 'SHARON', 'LaFRANIERE,', 'MARK', 'MAZZETTI', 'and', 'MATT', 'APUZZODEC.', '30,']\n",
      " \n",
      "6.txt\n",
      "['Advertisement', 'By', 'NICHOLAS', 'FANDOSDEC.', '30,', '2017', '', 'WASHINGTON', '—', 'A']\n",
      " \n",
      "7.txt\n",
      "['Advertisement', '', 'On', 'Washington', '', 'By', '', '', '', '']\n",
      " \n",
      "8.txt\n",
      "['Advertisement', 'By', 'NOAM', 'SCHEIBERDEC.', '31,', '2017', '', 'The', 'new', 'tax']\n",
      " \n",
      "9.txt\n",
      "['Advertisement', 'By', 'ADAM', 'LIPTAKDEC.', '31,', '2017', '', 'WASHINGTON', '—', 'Responding']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os, codecs\n",
    "\n",
    "for filename in os.listdir(\"../datasets/CleanedArticles/\"):  \n",
    "    if \".txt\" in filename:\n",
    "        print(filename)\n",
    "        \n",
    "        doc = codecs.open(\"../datasets/CleanedArticles/\"+filename, \"r\", \"utf-8\")\n",
    "        doc = doc.read()  \n",
    "        print (doc.split(\" \")[:10])\n",
    "        print (\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things-to-do:  \n",
    "\n",
    "This mission is similar to the previous one. \n",
    "What is the difference?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt\n",
      "1.txt\n",
      "10.txt\n",
      "11.txt\n",
      "12.txt\n",
      "13.txt\n",
      "15.txt\n",
      "16.txt\n",
      "18.txt\n",
      "19.txt\n",
      "2.txt\n",
      "3.txt\n",
      "4.txt\n",
      "5.txt\n",
      "6.txt\n",
      "7.txt\n",
      "8.txt\n",
      "9.txt\n"
     ]
    }
   ],
   "source": [
    "import os, codecs\n",
    "\n",
    "articles = []\n",
    "\n",
    "for filename in os.listdir(\"../datasets/CleanedArticles/\"):  \n",
    "    if \".txt\" in filename:\n",
    "        print(filename)\n",
    "        \n",
    "        doc = codecs.open(\"../datasets/CleanedArticles/\"+filename, \"r\", \"utf-8\")\n",
    "        doc = doc.read()  \n",
    "        articles.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'html_page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8736e6c2f246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# simply take all the paragraphs - we search for the elements \"p\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhtml_page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# we remove breaklines, tabs etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpara\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'html_page' is not defined"
     ]
    }
   ],
   "source": [
    "# we need a few libraries (library called 'os')\n",
    "# os is needed to loop over files in a folder\n",
    "# codecs is for encoding a file\n",
    "# BeautifulSoup is needed for parsing the html of a scraped page\n",
    "# incoding/decoding \n",
    "# for this block, this is for someone who really cares about HTML \n",
    "\n",
    "import codecs,os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we prepare an empty list\n",
    "articles = []\n",
    "\n",
    "for filename in os.listdir(\"../datasets/CleanedArticles/\"):  \n",
    "# for all the document (filename) which are in the libary os \n",
    "# and access to the directory\n",
    "    print(filename)\n",
    "    break \n",
    "\n",
    "# we loop over a folder\n",
    "\n",
    "    # we check if the file is a txt file\n",
    "    \n",
    "        # we open and read the file\n",
    "        \n",
    "        # look at the original HTML - we remove the internet archive toolbar\n",
    "    html_page = str(doc).split(\"<!-- END WAYBACK TOOLBAR INSERT -->\")[1]\n",
    "            \n",
    "        # we parse the page\n",
    "    html_page = BeautifulSoup(html_page, \"lxml\")\n",
    "        \n",
    "        \n",
    "        # we open a new file in writing mode (using codecs) / we need to create the \"CleanedArticles\" folder if it's not there\n",
    "    output = codecs.open(\"../datasets/CleanedArticles/\"+filename,\"w\",\"utf-8\")\n",
    "        \n",
    "        # we define a new list, called text\n",
    "    text = []\n",
    "\n",
    "        # simply take all the paragraphs - we search for the elements \"p\"\n",
    "for para in html_page.find_all('p'):\n",
    "            # we remove breaklines, tabs etc\n",
    "    para = para.text.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \")\n",
    "    text.append(para)\n",
    "        \n",
    "        # we put all paragraphs in a single text\n",
    "    text = \" \".join(text)\n",
    "        \n",
    "        # we write the text to the output file\n",
    "    output.write(text)\n",
    "        # we close the output file\n",
    "    output.close()\n",
    "        # we add the text to a list of articles\n",
    "    articles.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things-to-do: Tokenization \n",
    "## dissecting an text for further analysis\n",
    "\n",
    "## Step 1: open and read the target text  \n",
    "We are going to open a text file (\"15.txt\") in specified folder. \n",
    "Here we open and read the article under the variable name 'article' \n",
    "\n",
    "In order to TOKENIZE what we read, we need another library/package \n",
    "called 'nltk'(one of the most used text processing library in python) \n",
    "*NLTK stands for 'natural language toolkit' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertisement By NIKI KITSANTONISDEC. 30, 2017  ATHENS — An independent Greek government committee decided on Saturday to grant political asylum to a Turkish officer who fled last year’s failed coup in his country. The decision provoked an immediate angry response from Turkey, and hours later the Greek prime minister’s administration moved to block it. The officer was one of eight Turkish soldiers who fled to Greece in a military helicopter in July 2016 after the Turkish president, Recep Tayyip Erdogan, thwarted the attempted coup. On Saturday, the Greek committee for asylum requests ruled in favor of granting the officer asylum after determining there was no evidence implicating him in the coup. It also said the human rights situation in Turkey raised questions about his safety should he return to his homeland. The committee has yet to rule on the other seven, who have also applied for asylum. The office of Prime Minister Alexis Tsipras moved immediately to file an appeal against the decision, in an apparent effort to avoid further antagonizing Turkey. The committee’s decision put Mr. Tsipras in a difficult political position: While the prime minister has said that Greece does not support those involved in the coup, he has also voiced support for the independence of Greece’s judicial system. Advertisement The appeal was in line with the government’s “firm position regarding the eight servicemen, as it has been repeatedly and publicly expressed,” the prime minister’s office said in a statement.  Please verify you're not a robot by clicking the box. Invalid email address. Please re-enter. You must select a newsletter to subscribe to. View all New York Times newsletters. Turkey’s Foreign Ministry on Saturday described the decision as “politically motivated” and warned that it would hurt relations between the two countries. Greece “failed to show the support and cooperation we expect from an ally in the fight against terrorism and criminality,” the ministry said in a statement. During an official visit to Greece earlier this month, the first by a Turkish head of state in 65 years, Mr. Erdogan called for the extradition of the eight men, noting that Turkey is “not a country that tortures detainees.” He has also said that Mr. Tsipras had promised him the servicemen would be returned to Turkey just a few days after they landed on Greek soil. Advertisement By appealing the decision, Mr. Tsipras may be trying to avoid another political clash between the two neighbors, who have been divided by territorial disputes and ill will left over from the Turkish Ottoman Empire’s nearly 400-year occupation of Greece, which ended a century ago. Mr. Erdogan’s visit did little to improve relations between Turkey and Greece, which went to the brink of war in 1996 over a disputed islet in the Aegean Sea. We’re interested in your feedback on this page. Tell us what you think. Go to Home Page »\n",
      "[nltk_data] Downloading package punkt to /Users/shinnahee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = codecs.open(\"../datasets/CleanedArticles/12.txt\",\"r\",\"utf-8\")\n",
    "article = article.read()\n",
    "print (article)\n",
    "\n",
    "import nltk # --> documentation: http://www.nltk.org/\n",
    "\n",
    "# you will also need this\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: dissect into sentences\n",
    "## How to: nltk.sent_tokenize (variable_name)\n",
    "\n",
    "Now that we have an entire article, we are ready to tokenize this. \n",
    "Here before we break the article into words, let us break them by sentences. And in order to do that, we use 'nltk.sent_tokenize' \n",
    "*NOTE: the most important thing is to decide our breaking point, which could vary depending on our data processing and research question. \n",
    "\n",
    "And later we will use 'nltk.word_tokenize' to break sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Advertisement By JACEY FORTINDEC.\n",
      " \n",
      "31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage.\n",
      " \n",
      "(Sydney was among the first major cities to celebrate with fireworks at the stroke of midnight.)\n",
      " \n",
      "In Japan, people paraded in fox masks to attend the first prayer of the year at a Shinto shrine in Tokyo.\n",
      " \n",
      "In the Philippines, revelers gathered — phones in hand — at the Eastwood Mall in Manila to watch balloons and confetti rain down at midnight.\n",
      " \n",
      "Big pots of tea were prepared for New Year’s Eve celebrations in Beijing.\n",
      " \n",
      "The country will also celebrate the Lunar New Year, in February.\n",
      " \n",
      "It was raining in Singapore, but New Year’s Eve celebrants sheltered under umbrellas and raincoats as fireworks sparkled overhead.\n",
      " \n",
      "Tourists donned party hats to watch fireworks in front of the famous Petronas Twin Towers in Kuala Lumpur, Malaysia.\n",
      " \n",
      "Hundreds of couples got married at a mass wedding in Jakarta on New Year’s Eve.\n",
      " \n",
      "We’re interested in your feedback on this page.\n",
      " \n",
      "Tell us what you think.\n",
      " \n",
      "Go to Home Page »\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# we start by dividing the text into sentences\n",
    "sentences = nltk.sent_tokenize(article) \n",
    "print (type(sentences))\n",
    "# <-- documentation for this command: http://www.nltk.org/_modules/nltk/tokenize.html\n",
    "\n",
    "# let's print all the sentences, so we can exam the quality of the output\n",
    "for sentence in sentences: \n",
    "    print(sentence)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage.\n"
     ]
    }
   ],
   "source": [
    "# let's consider a single sentence - how do we do that? \n",
    "\n",
    "sentence = sentences[1]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: dissect a sentence into words \n",
    "## How to: nltk.word_tokenize (variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', ',', '2017', 'In', 'Sydney', ',', 'rainbow', 'fireworks', 'sparkled', 'off', 'the', 'Harbour', 'Bridge', 'in', 'celebration', 'of', 'Australia', '’', 's', 'recent', 'legalization', 'of', 'gay', 'marriage', '.']\n"
     ]
    }
   ],
   "source": [
    "# let's divide the sentence in tokens (aka single words)\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: lowering cases \n",
    "## How to: new_variable_name = [x.lower() for x in variable_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', ',', '2017', 'in', 'sydney', ',', 'rainbow', 'fireworks', 'sparkled', 'off', 'the', 'harbour', 'bridge', 'in', 'celebration', 'of', 'australia', '’', 's', 'recent', 'legalization', 'of', 'gay', 'marriage', '.']\n"
     ]
    }
   ],
   "source": [
    "# lower-casing the sentence\n",
    "without_capital_letters = [x.lower() for x in tokenized_sentence]\n",
    "print (without_capital_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework: write a for-loop for doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# homework: download stopwords <- google it out\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# what is \"stop\" ?\n",
    "\n",
    "print (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_stop_words = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework: how do we exclude punctuation? and numbers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
