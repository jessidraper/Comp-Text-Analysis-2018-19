{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('31', 'CD'), (',', ','), ('2017', 'CD'), ('In', 'IN'), ('Sydney', 'NNP'), (',', ','), ('rainbow', 'NN'), ('fireworks', 'NNS'), ('sparkled', 'VBD'), ('off', 'RP'), ('the', 'DT'), ('Harbour', 'NNP'), ('Bridge', 'NNP'), ('in', 'IN'), ('celebration', 'NN'), ('of', 'IN'), ('Australia', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('recent', 'JJ'), ('legalization', 'NN'), ('of', 'IN'), ('gay', 'JJ'), ('marriage', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# open the new dataset\n",
    "\n",
    "import codecs, nltk\n",
    "\n",
    "article = codecs.open(\"/Users/rebeccaweiss/Desktop/Comp-Text-Analysis-2018-19-master/datasets/CleanedArticles/15.txt\",\"r\",\"utf-8\")\n",
    "article = article.read()\n",
    "\n",
    "# split into sentences\n",
    "sentences = nltk.sent_tokenize(article) \n",
    "\n",
    "# take one single sentence \n",
    "\n",
    "sentence = sentences[1]\n",
    "\n",
    "#tokenize it\n",
    "\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "# you use the pos-tagger (it gives you back a list of tuples (word,pos))\n",
    "pos_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "print (pos_sentence)\n",
    "#need entire sentence, shouldn't/don't remove stopwords, need things like \"the\" to help recognize nouns\n",
    "\n",
    "#if want to be able to read, so NNP = proper noun, look at documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', ',', '2017', 'In', 'Sydney', ',', 'rainbow', 'firework', 'sparkle', 'off', 'the', 'Harbour', 'Bridge', 'in', 'celebration', 'of', 'Australia', '’', 's', 'recent', 'legalization', 'of', 'gay', 'marriage', '.']\n"
     ]
    }
   ],
   "source": [
    "#for thing like sparkled, need POS before lemmatizer, or else won't know what to do with it\n",
    "# combining lemmatization and pos tagging\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #define lemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer() #this is our lemmatizer, need to write this way because later\n",
    "#will write a command\n",
    "\n",
    "lemma_words = [] #define an empty list\n",
    "\n",
    "for word,pos in pos_sentence: #loop over part of speech tag, each element in list consists of token \n",
    "    #and part of speech\n",
    "    \n",
    "    #could be word or a noun, common thing/disambiguity\n",
    "    # if it's a verb - then we tell that to the lemmatizer\n",
    "    if pos[0] == \"V\": #if part of speech starts with a V\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word,\"v\") #then lemmatize it as a word, just say word, \"v\"\n",
    "        #there is a difference in capital and lower case, lemmatizer wants lower case of everything\n",
    "        #lemmatizer assumes things are noun if things are undecided, so if something is a verb, need to\n",
    "        #say it\n",
    "        \n",
    "        #if have disambuity with adjectives, then do it with adjectives\n",
    "        #but most problems when  dealing with a verb\n",
    "    else:\n",
    "    # otherwise, work as usual\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word) #lemmatizer gives you back a string\n",
    "    # we append the results\n",
    "    lemma_words.append(lemma) #puts the string into a list\n",
    "    \n",
    "print (lemma_words)\n",
    "\n",
    "#Note: this can be very slow if lemmatizing all speeches in a corpus\n",
    "\n",
    "#if want to keep sparkled in current form, then can keep as is above in POS, so list of words and\n",
    "#the part of speech they have, if want that information\n",
    "\n",
    "#always depends on what you want to do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now define a function that does all we need\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word_list = stopwords.words('english') #call stopword list if want to remove stopwords\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text): #this function takes text as a string as an input\n",
    "    #everytime write a function, write somewhere what type of object the function takes\n",
    "    \n",
    "    #put all the commands in a function, don't have to write a for loop every time\n",
    "    #nice way to code and present your code, and can easily check bugs\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token,\"v\") if pos[0] == \"V\" else wordnet_lemmatizer.lemmatize(token) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "    #can keep using the same name for text over and over again, just need to make sure not missing\n",
    "    #any steps\n",
    "    return text\n",
    "\n",
    "#can also put a counter that tells you how long it will take to process something, esp if \n",
    "#something is really long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage.\n",
      "['In', 'Sydney', 'rainbow', 'firework', 'sparkle', 'Harbour', 'Bridge', 'celebration', 'Australia', 'recent', 'legalization', 'gay', 'marriage']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "clean_sentence = nlp_pipeline(sentence) #use it this way\n",
    "#function name(what you want to put into the function)\n",
    "print (clean_sentence)\n",
    "\n",
    "#always double check, check over and over to make sure does what want, then use it for everything\n",
    "#then be very clear on what you did and how you did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Advertisement', 'By', 'JACEY', 'FORTINDEC', 'In', 'Sydney', 'rainbow', 'firework', 'sparkle', 'Harbour', 'Bridge', 'celebration', 'Australia', 'recent', 'legalization', 'gay', 'marriage', 'Sydney', 'among', 'first', 'major', 'city', 'celebrate', 'firework', 'stroke', 'midnight', 'In', 'Japan', 'people', 'parade', 'fox', 'mask', 'attend', 'first', 'prayer', 'year', 'Shinto', 'shrine', 'Tokyo', 'In', 'Philippines', 'reveler', 'gather', 'phone', 'hand', 'Eastwood', 'Mall', 'Manila', 'watch', 'balloon', 'confetti', 'rain', 'midnight', 'Big', 'pot', 'tea', 'prepare', 'New', 'Year', 'Eve', 'celebration', 'Beijing', 'The', 'country', 'also', 'celebrate', 'Lunar', 'New', 'Year', 'February', 'It', 'rain', 'Singapore', 'New', 'Year', 'Eve', 'celebrant', 'shelter', 'umbrella', 'raincoat', 'firework', 'sparkle', 'overhead', 'Tourists', 'party', 'hat', 'watch', 'firework', 'front', 'famous', 'Petronas', 'Twin', 'Towers', 'Kuala', 'Lumpur', 'Malaysia', 'Hundreds', 'couple', 'get', 'marry', 'mass', 'wedding', 'Jakarta', 'New', 'Year', 'Eve', 'We', 'interested', 'feedback', 'page', 'Tell', 'u', 'think', 'Go', 'Home', 'Page']\n"
     ]
    }
   ],
   "source": [
    "# let's take an entire article and use our pipeline!\n",
    "\n",
    "clean_article = nlp_pipeline(article)\n",
    "print (clean_article)\n",
    "#for something like topic detection, this is way more than fine\n",
    "#not super useful if want to do something more fine grained\n",
    "\n",
    "#not really great for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 4\n",
      "prayer 5\n",
      "raincoat 1\n",
      "party 6\n",
      "also 1\n",
      "JACEY 0\n",
      "Mall 2\n",
      "Eastwood 0\n",
      "Australia 2\n",
      "hand 16\n",
      "Tourists 1\n",
      "New 12\n",
      "recent 3\n",
      "pot 10\n",
      "By 2\n",
      "legalization 1\n",
      "u 4\n",
      "gay 7\n",
      "shrine 2\n",
      "attend 5\n",
      "stroke 16\n",
      "marriage 4\n",
      "Sydney 1\n",
      "midnight 1\n",
      "country 5\n",
      "Go 35\n",
      "Home 17\n",
      "prepare 8\n",
      "overhead 9\n",
      "famous 1\n",
      "It 1\n",
      "city 3\n",
      "among 0\n",
      "Page 9\n",
      "confetti 1\n",
      "Petronas 0\n",
      "Bridge 12\n",
      "feedback 2\n",
      "We 0\n",
      "Year 4\n",
      "rain 4\n",
      "mask 9\n",
      "celebration 3\n",
      "Kuala 0\n",
      "Twin 9\n",
      "reveler 1\n",
      "celebrant 2\n",
      "Jakarta 1\n",
      "first 16\n",
      "Philippines 3\n",
      "parade 5\n",
      "balloon 4\n",
      "Lumpur 0\n",
      "Shinto 3\n",
      "get 37\n",
      "people 6\n",
      "celebrate 3\n",
      "February 1\n",
      "FORTINDEC 0\n",
      "In 7\n",
      "shelter 7\n",
      "hat 4\n",
      "gather 11\n",
      "tea 5\n",
      "couple 9\n",
      "rainbow 2\n",
      "fox 10\n",
      "Towers 4\n",
      "Tell 9\n",
      "Eve 4\n",
      "interested 5\n",
      "Beijing 1\n",
      "Hundreds 1\n",
      "page 9\n",
      "wedding 5\n",
      "major 13\n",
      "Harbour 6\n",
      "Advertisement 1\n",
      "Manila 2\n",
      "firework 1\n",
      "umbrella 4\n",
      "Malaysia 1\n",
      "Japan 5\n",
      "Tokyo 1\n",
      "marry 2\n",
      "Singapore 3\n",
      "watch 13\n",
      "sparkle 7\n",
      "The 0\n",
      "phone 4\n",
      "front 13\n",
      "mass 11\n",
      "think 14\n",
      "Lunar 1\n",
      "Big 17\n"
     ]
    }
   ],
   "source": [
    "# word sense disambiguation\n",
    "\n",
    "# check documentation: http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn #how to use wordnet library in nltk\n",
    "\n",
    "#can get info on different sense of each token you have\n",
    "\n",
    "#want to get all the words in an article for wordnet, but want to get unique words\n",
    "\n",
    "# let's isolate each word - you do this using a set (another type of object in python)\n",
    "\n",
    "unique_words = set(clean_article) #one instance for each word - might want to lower case, or else\n",
    "#some inconsistencies\n",
    "\n",
    "#using a set, get one single instance of each word\n",
    "\n",
    "#print(unique_words)\n",
    "\n",
    "# let's check how many senses each word has\n",
    "for word in unique_words:\n",
    "    print (word, len(wn.synsets(word))) #for each word, how many sense it has\n",
    "    #synset is a list of sense, and each sense has different synonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "definition\n",
      "any small compartment\n",
      "\n",
      "example\n",
      "['the cells of a honeycomb']\n",
      "\n",
      "hypernymy\n",
      "[Synset('compartment.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[]\n",
      "\n",
      "synonyms\n",
      "['cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "(biology) the basic structural and functional unit of all organisms; they may exist as independent units of life (as in monads) or may form colonies or tissues as in higher plants and animals\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('living_thing.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('akaryocyte.n.01'), Synset('archespore.n.01'), Synset('arthrospore.n.01'), Synset('arthrospore.n.02'), Synset('beta_cell.n.01'), Synset('blastema.n.01'), Synset('blastomere.n.01'), Synset('daughter_cell.n.01'), Synset('embryonic_cell.n.01'), Synset('fiber.n.03'), Synset('flagellated_cell.n.01'), Synset('gametocyte.n.01'), Synset('kupffer's_cell.n.01'), Synset('leydig_cell.n.01'), Synset('mother_cell.n.01'), Synset('parthenote.n.01'), Synset('plant_cell.n.01'), Synset('polar_body.n.01'), Synset('recombinant.n.01'), Synset('reproductive_cell.n.01'), Synset('sertoli_cell.n.01'), Synset('somatic_cell.n.01'), Synset('zygote.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a device that delivers an electric current as the result of a chemical reaction\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('electrical_device.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('electrolytic_cell.n.01'), Synset('fuel_cell.n.01'), Synset('solar_cell.n.01'), Synset('storage_cell.n.01'), Synset('voltaic_cell.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell', 'electric_cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a small unit serving as part of or as the nucleus of a larger political movement\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('political_unit.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('sleeper_cell.n.01'), Synset('terrorist_cell.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell', 'cadre']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a hand-held mobile radiotelephone for use in an area divided into small sections, each with its own short-range transmitter/receiver\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('radiotelephone.n.02')]\n",
      "\n",
      "hyponyms\n",
      "[]\n",
      "\n",
      "synonyms\n",
      "['cellular_telephone', 'cellular_phone', 'cellphone', 'cell', 'mobile_phone']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "small room in which a monk or nun lives\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('room.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[]\n",
      "\n",
      "synonyms\n",
      "['cell', 'cubicle']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "definition\n",
      "a room where a prisoner is kept\n",
      "\n",
      "example\n",
      "[]\n",
      "\n",
      "hypernymy\n",
      "[Synset('room.n.01')]\n",
      "\n",
      "hyponyms\n",
      "[Synset('bullpen.n.02'), Synset('cooler.n.03'), Synset('dungeon.n.02'), Synset('guardroom.n.01'), Synset('hold.n.07'), Synset('sweatbox.n.01')]\n",
      "\n",
      "synonyms\n",
      "['cell', 'jail_cell', 'prison_cell']\n",
      "\n",
      "antonyms\n",
      "[]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"cell\" #can also use other things aside from wordnets, like thesaurus, sometimes useful\n",
    "\n",
    "#can also search it on wordnetweb website, see if it gives you stuff you care about\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    print (\"\\ndefinition\")\n",
    "    print(sense.definition())\n",
    "    \n",
    "    # get a textual example\n",
    "    print (\"\\nexample\")\n",
    "    print(sense.examples())\n",
    "    \n",
    "    # get hypernymy\n",
    "    print (\"\\nhypernymy\")\n",
    "    print(sense.hypernyms())\n",
    "\n",
    "    # get hyponyms\n",
    "    print (\"\\nhyponyms\")\n",
    "    print(sense.hyponyms())\n",
    "        \n",
    "    # this is a way of getting synonyms - there are others\n",
    "    print (\"\\nsynonyms\")\n",
    "    print (sense.lemma_names())\n",
    "    \n",
    "    # this is for getting antonyms - works especially with adjectives \n",
    "    print (\"\\nantonyms\")\n",
    "    print (sense.lemmas()[0].antonyms())\n",
    "    \n",
    "    print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean sent 1: ['The', 'terrorist', 'cell', 'neutralize', 'near', 'southern', 'Russian', 'city', 'Makhachkala', 'capital', 'Republic', 'Dagestan']\n",
      "clean sent 2: ['The', 'molecule', 'use', 'light', 'energy', 'move', 'proton', 'across', 'somatic', 'cell', 'membrane', 'prove', 'unsuitable', 'crystallography']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# let's consider two sentences where \"cell\" is mentioned\n",
    "\n",
    "sent1 = \"The terrorist cell was neutralized near the southern Russian city of Makhachkala, the capital of the Republic of Dagestan.\"\n",
    "\n",
    "sent2 = \"The molecule, which uses light energy to move protons across a somatic cell membrane, proved unsuitable for crystallography.\"\n",
    "\n",
    "# you clean the sentences using our pipeline\n",
    "clean_sent1 = nlp_pipeline(sent1)\n",
    "\n",
    "clean_sent2 = nlp_pipeline(sent2)\n",
    "\n",
    "print (\"clean sent 1:\", clean_sent1)\n",
    "print (\"clean sent 2:\", clean_sent2)\n",
    "print (\" \")\n",
    "\n",
    "#compare the two sentences, and find most similar one, do this for homework\n",
    "# for each possible sense of \"cell\" you can, for instance, check the overlap between \n",
    "#the definition and the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to write function\n",
    "#compare the definition with the context, and try to find a way of counting how many words in \n",
    "#definition are in common with the context, counting how many words they have in common\n",
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework: find the best sense - \n",
    "#implement your version of the Lesk algorithm: https://en.wikipedia.org/wiki/Lesk_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#potential homework 2: get the json fie with the tweets from donald trump and\n",
    "#improve his vocab by changing his poor choice of adjectives with\n",
    "#more sophisticated synonyms\n",
    "\n",
    "#or\n",
    "\n",
    "#make his tweets nicer by changing adjectives related to antonyms\n",
    "\n",
    "#or combine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
